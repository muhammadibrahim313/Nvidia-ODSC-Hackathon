{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Curation\n",
    "This notebook showcases the building blocks that can be used for building a simple data curation pipeline using [NeMo Curator](https://github.com/NVIDIA/NeMo-Curator).\n",
    "\n",
    "## Reading Materials\n",
    "Before proceeding, we highly recommend looking through the following deep dive blog posts that walk you through building data curation pipelines using NeMo Curator:\n",
    "- [Curating Custom Datasets for LLM Training with NVIDIA NeMo Curator](https://developer.nvidia.com/blog/curating-custom-datasets-for-llm-training-with-nvidia-nemo-curator/)\n",
    "- [Curating Custom Datasets for LLM Parameter-Efficient Fine-Tuning with NVIDIA NeMo Curator](https://developer.nvidia.com/blog/curating-custom-datasets-for-llm-parameter-efficient-fine-tuning-with-nvidia-nemo-curator/)\n",
    "\n",
    "Also, please checkout [our tutorials](https://github.com/NVIDIA/NeMo-Curator/tree/main/tutorials) in the repository to learn more about various functionalities that NeMo Curator provides.\n",
    "\n",
    "In this notebook, we will use the [Law-StackExchange dataset](https://huggingface.co/datasets/ymoslem/Law-StackExchange) for this pipeline, which is a dataset of legal question/answers scraped from the Stack Exchange website. This notebook is the summarized version of our existing [synthetic data generation tutorial](https://github.com/NVIDIA/NeMo-Curator/tree/main/tutorials/peft-curation-with-sdg). Feel free to go through that tutorial to gain a better understanding of various NeMo Curator facilities.\n",
    "\n",
    "## Setup and Requirements\n",
    "The NeMo dependencies are already installed in the container. However, before proceeding you need to install one dependency to follow along. Execute the following cell before getting started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, let's setup some environment variables, as well as path variables that will be used for storing the curated data, as well as intermediate temporary files that are required for this notebooks to function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DASK_DATAFRAME__QUERY_PLANNING\"] = \"False\"  # Needed for running Curator on the GPU\n",
    "\n",
    "NOTEBOOK_DIR = os.path.abspath(\"\")\n",
    "DATA_DIR = os.path.join(NOTEBOOK_DIR, \"data\")\n",
    "TEMP_DIR = os.path.join(NOTEBOOK_DIR, \".temp\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now import everything we need to build our data curation pipeline. For your conveniene, we've provided the document builder implementations that allow you to download the dataset from HuggingFace and convert it into a Pandas `DataFrame`.\n",
    "\n",
    "We have additionally implemented a score-based filter that allows you to filter the dataset rows using the score values assigned to each question. You can use this implementation as the basis for creating your own filtering/scoring mechanisms using NeMo Curator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.utils.distributed_utils import get_client\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "from nemo_curator.filters import WordCountFilter\n",
    "from nemo_curator.modifiers import UnicodeReformatter\n",
    "from nemo_curator.utils.file_utils import expand_outdir_and_mkdir\n",
    "from nemo_curator import ScoreFilter, Sequential\n",
    "from nemo_curator.modules.modify import Modify\n",
    "\n",
    "# Importing helper functions\n",
    "from helpers.filters import FilterLowScores\n",
    "from helpers.docbuilder import download_and_convert_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, let's decide the compute resources we'd like to use for running our data curation pipeline. NeMo Curator uses Dask to orchestrate scalable data processing. As such, it needs to know what resources to use. \n",
    "\n",
    "For the purposes of this notebook, we will instruct NeMo Curator to use 8 CPU workers. While most NeMo Curator functionalities can be executed on the CPU, some modules (such as semantic deduplication) can only be executed on the GPU. Please make sure to select the appropriate device.\n",
    "\n",
    "Note that you can increase or decrease the number of CPU workers depending on the runtime environment. Keep in mind that each CPU worker gets allocated a fixed amount of the total available system memory (RAM). Thus, if the environment does not have enough memory available, Dask operations might fail.\n",
    "\n",
    "Once we have decided on the resources to use, we can initialize our Dask cluster and start using NeMo Curator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"  # It can be either \"cpu\" or \"gpu\"\n",
    "n_workers = 4  # Number of workers to use for Dask. If running out of memory, try reducing this.\n",
    "client = get_client(device, n_workers=n_workers, set_torch_to_use_rmm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Main Data Curation and Processing Pipeline\n",
    "\n",
    "We start by downloading and converting the dataset into a suitable format. This is done via the document builders that we have provided for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = download_and_convert_dataset(DATA_DIR)\n",
    "raw_dataset = DocumentDataset.from_pandas(dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define our data curation pipeline. The pipeline we define here is very simple, as it contains basic filtering operations\n",
    "\n",
    "> NOTE: to use the modules that need a GPU, the dataset has to be converted to the `cudf` backend. Please refer to [this tutorial](https://github.com/NVIDIA/NeMo-Curator/tree/main/tutorials/peft-curation-with-sdg) for an example demonstrating the usage of GPU modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_curation_pipeline(dataset: DocumentDataset, device: str) -> DocumentDataset:\n",
    "    print(f\"Running curation pipeline on '{device}'...\")\n",
    "    orig_dataset = dataset\n",
    "\n",
    "    cpu_curation_steps = Sequential(\n",
    "        [\n",
    "            #\n",
    "            # Modifications\n",
    "            #\n",
    "            # Unify the text encoding to Unicode.\n",
    "            Modify(UnicodeReformatter(), text_field=\"title\"),\n",
    "            Modify(UnicodeReformatter(), text_field=\"question\"),\n",
    "            #\n",
    "            # Filtering\n",
    "            #\n",
    "            # Filter out records based on the question word counts.\n",
    "            ScoreFilter(\n",
    "                WordCountFilter(min_words=50, max_words=500),\n",
    "                text_field=\"question\",\n",
    "                score_type=int,\n",
    "            ),\n",
    "            # Filter out records where the question has a negative score.\n",
    "            ScoreFilter(\n",
    "                FilterLowScores(score_threshold=0),\n",
    "                text_field=\"question_score\",\n",
    "                score_type=bool,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Run the CPU curation steps.\n",
    "    dataset = cpu_curation_steps(dataset)\n",
    "    dataset = dataset.persist()\n",
    "    # Drop the columns that are no longer needed.\n",
    "    dataset.df = dataset.df.drop(columns=[\"answer\", \"answer_score\", \"question_score\"])\n",
    "    orig_len = len(orig_dataset.df)\n",
    "    new_len = len(dataset.df)\n",
    "\n",
    "    print(f\"Original dataset length: {orig_len}\")\n",
    "    print(f\"New dataset length: {new_len}\")\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to run the pipeline and get our final dataset. This may take up to 10 minutes to execute, especially if any GPU functionalities are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_dataset = run_curation_pipeline(raw_dataset, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's specify the final columns that we would like our dataset to have. Depending on how you plan on consuming this dataset for training, you may decide to introduce other arbitrary columns to help the model learn better.\n",
    "\n",
    "Also, this is a great place to add system or instruction prompts to every record, in case you intend to use the same instruction prompt for every record.\n",
    "\n",
    "Let's define a function that formats the dataset, and also adds system prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(dataset: DocumentDataset, filename: str) -> DocumentDataset:\n",
    "    SYSTEM_PROMPT = \"Read the following title and question about a legal issue and assign the most appropriate tag to it. All tags must be in lowercase, ordered lexicographically and separated by commas.\\n\\n\"\n",
    "\n",
    "    df = dataset.df.compute()\n",
    "    has_tags = \"tags\" in df.columns\n",
    "    df[\"input\"] = SYSTEM_PROMPT + \"TITLE:\\n\" + df[\"title\"] + \"\\n\\n\" + \"QUESTION:\\n\" + df[\"question\"]\n",
    "    df[\"output\"] = df[\"tags\"] if has_tags else \"\"  # If the dataset doesn't have tags, use an empty string.\n",
    "    df[\"filename\"] = filename\n",
    "\n",
    "    df = df.drop(columns=[\"title\", \"question\"])\n",
    "    if has_tags:\n",
    "        df = df.drop(columns=[\"tags\"]) # Drop the tags column if it exists.\n",
    "    return DocumentDataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the function above to format the dataset. We apply the same logic to the final evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_dataset = format_dataset(curated_dataset, \"law-stackexchange-curated.jsonl\")\n",
    "print(f\"Original dataset columns: {curated_dataset.df.columns}\")\n",
    "print(f\"Formatted dataset columns: {formatted_dataset.df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the final dataset is ready, we can write it into a JSONL file that is in the format expected for training with NeMo Framework.\n",
    "\n",
    "> NOTE: The curated dataset will be written under `curator/data/curated_dataset/law-stackexchange-curated.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Curated dataset columns: {formatted_dataset.df.columns}\")\n",
    "result_fp = os.path.join(DATA_DIR, \"curated_dataset\")\n",
    "print()\n",
    "print(f\"Saving curated dataset to '{result_fp}'...\")\n",
    "formatted_dataset.to_json(result_fp, write_to_filename=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Spliting the Dataset\n",
    "\n",
    "Before starting the model training procedure, let's split the dataset we've just curated into `training`, `validation` and `test` splits with 80/10/10 ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "VAL_RATIO = 0.05\n",
    "\n",
    "df = formatted_dataset.df.compute()\n",
    "\n",
    "# Some sanity checks\n",
    "assert len(df) > 0, \"The dataset is empty.\"\n",
    "assert VAL_RATIO >= 0 and VAL_RATIO <= 1, \"VAL_RATIO must be between 0 and 1.\"\n",
    "val_size = int(len(df) * VAL_RATIO)\n",
    "output_dir = f\"{DATA_DIR}/split\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Split the data into training and temporary sets\n",
    "train_df, val_df = train_test_split(df, test_size=val_size, random_state=42)\n",
    "\n",
    "print(f\"Original size: {len(df)}\")\n",
    "print(\"After splitting:\")\n",
    "print(f\"    Train size: {len(train_df)}\")\n",
    "print(f\"    Validation size: {len(val_df)}\")\n",
    "\n",
    "train_df[\"filename\"] = \"train.jsonl\"\n",
    "val_df[\"filename\"] = \"val.jsonl\"\n",
    "\n",
    "DocumentDataset.from_pandas(train_df).to_json(output_dir, write_to_filename=True)\n",
    "DocumentDataset.from_pandas(val_df).to_json(output_dir, write_to_filename=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Preparing the Submission Dataset\n",
    "\n",
    "The submission dataset is dataset of questions and titles, where every participating team would have to predict the tags for.\n",
    "It needs to have a format similar to training datasets so that you can evaluate your model on it, and submit your predicted tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_ds = \"data/submission/evaluation-dataset-verified-for-participants.jsonl\"\n",
    "assert os.path.exists(submission_ds), f\"The submission dataset does not exist at '{submission_ds}'\"\n",
    "submission_ds = DocumentDataset.read_json(submission_ds)\n",
    "submission_ds = format_dataset(submission_ds, \"submission.jsonl\")\n",
    "print(\"Writing the formatted submission dataset to disk...\")\n",
    "submission_ds.to_json(output_dir, write_to_filename=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have run the above cell, your data that is suitable for training will be written under `data/split`. When making submissions, run inference with your model on `data/split/submission.jsonl`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Freeing Memory and Other Resources\n",
    "\n",
    "Before moving to the next notebook, please execute the following cell to free up all the allocated resources to avoid running into out-of-memory or other issues.\n",
    "\n",
    "Alternatively, please restart the kernel by navigating to `Kernel > Restart Kernel` (if using Jypyter notebook), or clicking the `Restart` button in VS Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
